%----------------------------------------------------------------------------------------
%	Information Theory
%----------------------------------------------------------------------------------------
\section{Information Theory}

Information theory deals with quantifying information. It was developed by Claude Shannon with the goal of establishing limits on signal processing operations such as data compression, reliable storage, and communcation (cite wiki). 

The basic unit of information is the \textbf{bit}, a portmanteau of binary digit. It can have values of 0 or 1, and could represent boolean states (false/true) or activation states (off/on) among other things. \textbf{Bit-length} refers to the length of a binary number. Table \ref{table:datatypes} lists common data types from the IEEE Standard For Floating Point Arithmetic (IEEE 754). 

\begin{tabular}{ l | c | l }
data type & bit length & bit assignment \\
\hline
signed int & 32 & Bit string with range [$-2^{31}, 2^{31}-1]$ \\
unsigned int & 32 & Bit string with range $[0, 2^{32}-1]$ \\
float & 32 & 1 sign, 8 exponent , 24 significand precision \\
double & 64 & 1 sign, 11 exponent, 53 significand precision \\ 
\label{table:datatypes}
\end{tabular}

%------------------------------------------------
% Subsection: Entropy
\subsection{Entropy}

Shannon entroy H - the average number of bits per symbol needed to store or communicate a message. "Bits assumes $\log_{2}$. "It is analogous to intensive physical entropies like molar and specific entropy $S_{0}$ but not extensive physical entropy $S$."

There's also entropy H times the number of symbols in a message. This measure of entropy is in bits. It is analogous to absolute (extensive) physical entropy S. 

Landauer's Principle: A change in H*N is equal to a change in physical entropy when the information system is perfectly efficient. 

What does entropy mean? It is a measure of the uncertainty involved in predicting the value of a random variable. Example: a coin flip with 50 \% probabilities of heads or tails provides less information than a die roll with a $\frac{1}{6}$ probability per side. 

The Shannon entropy H in units of bits per symbol is given by

\begin{equation}
H = - \sum_{i} f_{i} \log_{2} (f_{i}) = - \sum_{i} \log_{2} (\frac{c_{i}}{N}).
\end{equation}

In this equation, $i$ is an index over distinct symbols, $f_{i}$ is the frequency of symbols occurring in the message, and $c_{i}$ is the number of times the $i^{th}$ symbol occurs in the message of length $N$. $H N$ is the entropy of a message. This has units of bits (not bits per symbol). 

Example: a Bernoulli (binary) trial where $X=1$ occurs with some probability. The entropy $H(X)$ of the trial is maximized when the two possible outcomes are equally probable ($P(X=1) = 0.5$).

\begin{tabular}{l | l}
\textbf{Boltzmann H} & \textbf{Shannon H} \\
\hline
physical systems of particles & information systems of symbols \\
entropy per particle & entropy per symbol \\
probability of a microstate & probability of a symbol \\
Extensive physical entropy $S$ & Entropy of file or msssage $HN$ \\
\end{tabular}

%------------------------------------------------
% Subsection: Numerical Systems
\subsection{Numerical Systems}

We usually count in base 10 (maybe because we have 10 fingers and 10 toes?). But there are other counting systems available. For instance, computers are programmed with binary instructions because transistors can have "on" or "off" gate positions. Binary, octal, decimal, and hexadecimal are all examples of \textbf{positional numerical systems}. The \textbf{radix} refers to the number of symbols that can be used at each position in a number. For the decimal system, the radix is 10 because each decimal place can accomodate the numbers 0-9. The radix is 2 for binary, because each position can accomodate the numbers 0 or 1. For hexadecimal, it is 16 (0-9, as well as A-F). And so on... 

The equation below shows how to convert a hexadecimal (base 16) number into a decimal (base-10) number. 

\begin{equation}
	\begin{split}
	2 A F 3 & = (2_{16} \cdot 16^{3}) + (A_{16} \cdot 16^{2}) + (F_{16} \cdot 16^{1}) + (3_{16} \cdot 16^{0}) \\
	& = (2 \cdot 16^{3}) + (10 \cdot 16^{2}) + (15 \cdot 16^{1}) + (3 \cdot 16^{0}) \\
	& = 10995
	\end{split}
\end{equation}

Here's another example for converting from a decimal base to a binary base.

\begin{equation}
	\begin{split}
	1035 & = (1_{10} \cdot 10^{3}) + (0_{10} \cdot 10^{2}) + (3_{10} \cdot 10^{1}) + (5_{10} \cdot 10^{0}) \\
	& = (1 \cdot 2^{10}) + (1 \cdot 2^{3}) + (1 \cdot 2^{1}) + (1 \cdot 2^{0}) \\
	& = 10000001011
	\end{split}
\end{equation}


