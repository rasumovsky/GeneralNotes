%----------------------------------------------------------------------------------------
%	Information Theory
%----------------------------------------------------------------------------------------
\section{Information Theory}

Information theory deals with quantifying information. It was developed by Claude Shannon with the goal of establishing limits on signal processing operations such as data compression, reliable storage, and communcation (cite wiki). 

The basic unit of information is the \textbf{bit}, a portmanteau of binary digit. It can have values of 0 or 1, and could represent boolean states (false/true) or activation states (off/on) among other things. \textbf{Bit-length} refers to the length of a binary number. Table \ref{table:datatypes} lists common data types from the IEEE Standard For Floating Point Arithmetic (IEEE 754). 

\begin{tabular}{ l | c | l }
data type & bit length & bit assignment \\
\hline
signed int & 32 & Bit string with range [$-2^{31}, 2^{31}-1]$ \\
unsigned int & 32 & Bit string with range $[0, 2^{32}-1]$ \\
float & 32 & 1 sign, 8 exponent , 24 significand precision \\
double & 64 & 1 sign, 11 exponent, 53 significand precision \\ 
\label{table:datatypes}
\end{tabular}

\subsection{Entropy}

Shannon entroy H - the average number of bits per symbol needed to store or communicate a message. "Bits assumes $\log_{2}$. "It is analogous to intensive physical entropies like molar and specific entropy $S_{0}$ but not extensive physical entropy $S$."

There's also entropy H times the number of symbols in a message. This measure of entropy is in bits. It is analogous to absolute (extensive) physical entropy S. 

Landauer's Principle: A change in H*N is equal to a change in physical entropy when the information system is perfectly efficient. 

What does entropy mean? It is a measure of the uncertainty involved in predicting the value of a random variable. Example: a coin flip with 50 \% probabilities of heads or tails provides less information than a die roll with a $\frac{1}{6}$ probability per side. 

The Shannon entropy H in units of bits per symbol is given by

\begin{equation}
H = - \sum_{i} f_{i} \log_{2} (f_{i}) = - \sum_{i} \log_{2} (\frac{c_{i}}{N}).
\end{equation}

In this equation, $i$ is an index over distinct symbols, $f_{i}$ is the frequency of symbols occurring in the message, and $c_{i}$ is the number of times the $i^{th}$ symbol occurs in the message of length $N$. $H N$ is the entropy of a message. This has units of bits (not bits per symbol). 

Example: a Bernoulli (binary) trial where $X=1$ occurs with some probability. The entropy $H(X)$ of the trial is maximized when the two possible outcomes are equally probable ($P(X=1) = 0.5$).

\begin{tabular}{l | l}
\textbf{Boltzmann H} & \textbf{Shannon H} \\
\hline
physical systems of particles & information systems of symbols \\
entropy per particle & entropy per symbol \\
probability of a microstate & probability of a symbol \\
Extensive physical entropy $S$ & Entropy of file or msssage $HN$ \\
\end{tabular}