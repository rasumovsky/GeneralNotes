%----------------------------------------------------------------------------------------
%	Information Theory
%----------------------------------------------------------------------------------------
\section{Information Theory}

Information theory deals with quantifying information. It was developed by Claude Shannon with the goal of establishing limits on signal processing operations such as data compression, reliable storage, and communcation (cite wiki). 

\subsection{Entropy}

Shannon entroy H - the average number of bits per symbol needed to store or communicate a message. "Bits assumes $\log_{2}$. "It is analogous to intensive physical entropies like molar and specific entropy $S_{0}$ but not extensive physical entropy $S$."

There's also entropy H times the number of symbols in a message. This measure of entropy is in bits. It is analogous to absolute (extensive) physical entropy S. 

Landauer's Principle: A change in H*N is equal to a change in physical entropy when the information system is perfectly efficient. 

What does entropy mean? It is a measure of the uncertainty involved in predicting the value of a random variable. Example: a coin flip with 50 \% probabilities of heads or tails provides less information than a die roll with a $\frac{1}{6}$ probability per side. 