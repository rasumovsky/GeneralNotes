%----------------------------------------------------------------------------------------
%	Deep Learning
%----------------------------------------------------------------------------------------
\section{Deep Learning}

This section discusses the basics of deep learning. The material is based largely on notes from the Google Udacity course "\href{https://www.udacity.com/course/deep-learning--ud730}{Deep Learning}". On a basic level, deep learning involves training multi-layer neural networks to transform an input data vector into a particular output vector. The networks can be trained in a supervised manner by providing the input data along with the desired output vector, or in an unsupervised way by training the network to build internal, compressed representations of the data in order to reconstruct the input data.

%------------------------------------------------
%Subsection: Softmax
\subsection{Softmax}

In neural networks and other machine learning algorithms, the \textbf{softmax} function is used as the final layer of a classification network. The softmax function is a generalization of the logistic function. It transforms a $K$-dimensional vector $\mathbf{z}$ of arbitrary real values into a $K$-dimensional vector $\sigma(\mathbf{z})$ of real values in the range $(0,1)$ which sum to 1. 

\begin{equation}
\sigma(\mathbf{z})_{j} = \frac{e^{z_{j}}}{\sum_{k=1}^{K} e^{z_{k}}} \text{, for j=1,...,K}
\end{equation}

The result of the softmax function can, for example, be used as the predicted probability for the $j^{th}$ class in a classification task.

Softmax score probabilities get close to 0 or 1 as scores get larger. Similarly, score probabilities get closer to uniform as all scores get smaller. A classifier is less confident in a result with low scores and more confident in a result with high scores. The training of a classifier is designed to increase confidence over time. 

%------------------------------------------------
%Subsection: Initializing variables
\subsection{Initializing variables}

Generally, it is useful to have variables with a mean near zero and equal variance. This will help avoid numerical instability issues and ill-conditioned matrices. Remember: matrix algebra really underpins all of this stuff!

Weight and bias initialization needs to be done carefully, otherwise the training of the model is inefficient. A useful method is to randomly sample from a Gaussian distribution centered at zero with a given standard deviation. The standard deviation is related to the order of magnitude of hte outputs. For a less certain initialization, it is a good idea to use a small standard deviation (much less than 0.1).

%------------------------------------------------
%Subsection: Cross-validation
\subsection{Cross-Validation}

\textbf{Cross-validation}, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent set of data. The idea is that the performance of a model needs to be measured using a dataset that is separate from the one used for training. If the same data are used for training and testing a model, the performance will be falsely inflated, since the model will have trained on some of the statistical variations in the training data. Cross-validation can prevent \textbf{over-fitting}.

A simple cross-validation system involves splitting up the data into a \textbf{training}, \textbf{validation}, and \textbf{testing} dataset. 

\begin{itemize}
	\item Training set: a data sample for training the neural network via back-propagation. 
	\item Validation set: a data sample for validating and measuring the performance.
	\item Testing set: a data sample for preventing over-training. 
\end{itemize}

The bigger the validation set, the more precise the estimate of the model performance. The performance measurement is susceptible to statistical fluctuations, after all. More data always helps. Having a large training dataset is the most important thing. 

%------------------------------------------------
%Subsection: Loss function
\subsection{Loss function}

Machine learning is often about desigining the correct loss-function to use for model optimization. 

In order to train a model, it is necessary to measure how far away the output of a model is from the desired output. One way to do this is to minimize the \textbf{cross-entropy}. Use the average cross-entropy as the loss function.

Sometimes it is difficult to scale the regression model, where the loss function and gradient is computed for each example. The gradient computation, for example, typically takes 3 times as long as the loss computation.

One solution, called \textbf{stochastic gradient descent}, 

%------------------------------------------------
%Subsection: Stochastic Gradient Descent
\subsection{Stochastic Gradient Descent}

Stochastic gradient descent (SGD) involves computing the loss for a tiny random sample of data instead of the whole dataset (between 1 and 1000 training samples each time). This method requires random sampling of the training data, otherwise it won't work. On average, the gradient will point in the right direction. But the approach requires many small steps, each of which might be partially misguided. 

Stochastic gradient descent is at the core of deep learning, since it \textit{scales very well} with data and model size. Bigger data and bigger models are better, after all. However, SGD is a fast but bad optimizer, and it comes with some issues.

First, in order to work properly, the input variables need to have a mean of zero and equal, small, variances. Similarly, the initial weights and biases should have a mean of zero and very small variance. 

The training algorithm can also use knowledge from previous steps to know where it should be going. This method, called \textbf{momentum}, keeps a running average of the gradient direction and uses that for optimization instead of the single gradient measurement. 

It is also useful to make smaller steps as the training continues. This procedure, called \textbf{learning rate decay}, lowers the learning rate over time, typically by exponential damping. 

The learning rate and momentum are examples of \textbf{hyper-parameters} that can be tuned for a model. Some other hyper-parameters include the initial learning rate, the batch size for SGD, and the weight initialization. \textbf{AdaGrad} is a modification of SGD that implicitly does momentum and learning rate decay. It makes learning less sensitive to hyper-parameters, but is slightly worse than tuned SGD with momentum. 

One useful suggestion: \textit{"Keep calm and lower your learning rate"}. 

%------------------------------------------------
%Subsection: Linear Models
\subsection{Linear Models}

A linear classifier creates a discriminant based on the value of a linear combination of the input features. For an input feature vector of real values, $\mathbf{x}$, the output score y is given by:

\begin{equation}
	y = f(\mathbf{w} \cdot \mathbf{x}) = f( \sum_{j} w_{j} x_{j}),
\end{equation}

In this expression, $\mathbf{w}$ is a real vector of weights and $f$ is a function that converts the dot-product of the two vectors into the desired output. $\mathbf{w}$ is a linear function mapping of $\mathbf{x}$ onto $\mathbb{R}$.

Numerically, linear operations are very stable. This means that small changes to the inputs do not lead to big changes in the outputs. The derivatives are constant, so they are also quite stable. The problem is that many problems cannot be represented by a linear model. For example, addition can be modeled, but multiplication cannot be modeled. 

The trick is to create a non-linear model from linear components. We want to keep our parameters inside big linear functions. After all, GPUs are designed for linear operations (e.g. matrix multiplication). And it is really useful to harness GPUs for training. 

%------------------------------------------------
%Subsection: Non-Linear Models
\subsection{Non-Linear Models}

One way to introduce non-linearities into a model is to use \textbf{rectified linear units}. A RELU returns 0 if $x<0$ and $x$ if $x \geq 0$. One advantage of RELU is that the derivative is constant.

A logistic model can then be extended by inserting a RELU layer in the middle: the first layer connects the input features to the RELU, while the second layer connects the RELU to the classifier. The resulting model is nonlinear. 

The first layer of the model consists of the set of weights and biases applied to the input features and passed through the RELUs. The output layer of the RELU layer is fed to the next one, but it is not observable from outside the network. Hence, it is known as a \textbf{hidden} layer. The second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities. 

A network can be built by stacking simple operations on top of each other. It makes the back-propagation math, which is based on the chain rule, very simple! 

%------------------------------------------------
%Subsection: Back-Propagation:
\subsection{Back-Propagation}

\textbf{Back-propagation} is the mechanism for using differences between the desired and actual model output to update the model parameters and improve the prediction. Basically, the loss function is used to compute a gradient for each model parameter using the chain rule. The model parameters (weights and biases) are then updated. As a reminder, the \textbf{chain rule} states that:

\begin{equation}
	[g(f(x))]' = g'(f(x)) \cdot f'(x).
\end{equation}

Note that each block of the back-propagation takes twice the memory and twice the computation time of the forward-propagation. 

%------------------------------------------------
%Subsection: Deep Models
\subsection{Deep Models}

The accuracy of a model can improved by adding more parameters to the model. This could be done by increasing the size of each layer of the model, but this turns out to be inefficient. Increasing the number of nodes per layer makes the network harder to train. A better approach is to make the model deeper by adding more layers. It is more efficient in terms of the number of parameters; you can get more performance with fewer parameters by going deeper rather than wider. Furthermore, deep models capture data with hierarchical structures better. Deeper layers are able to learn more abstract features of the data. 

However, deep models only shine if you have enough data to train them. But it is hard to optimize a network that is the right size. So it is often easier to train a big network and then try to prevent over-fitting. This is sometimes called the "skinny jeans" problem. Skinny jeans look great, but it is hard to fit in them. So it is usually easier to wear jeans that are just a bit too big. 

%------------------------------------------------
%Subsection: Preventing Over-Fitting
\subsection{Preventing Over-Fitting}

One way to prevent over-fitting is \textbf{early termination}. During training, keep track of the validation set performance versus training time. As soon as improvement stops being made on the validation set performance, terminate the training. 

Another way to prevent overfitting is through \textbf{regularization}. With regularization, artificial constraints are applied to the network that implicitly reduce the number of free parameters while not making it more difficult to optimize. 

\textbf{L2 regularization} adds a new term to the loss function that penalizes large weights. The new term is just the sum of the square of the weights multiplied by a new hyperparameter constant $c$: 

\begin{equation}
	Loss' = Loss + \frac{c}{2} (w_{1}^{2} + w_{2}^{2} + ... + w_{M}^{2}).
\end{equation}

L2 regularization is a simple technique that does not change the structure of the network. Furthermore, the derivative of teh L2 norm of the vector is just the weights:

\begin{equation}
	W = w_{1} + w_{2} + ... + w_{M}.
\end{equation}

Perhaps the most important recent technique is called \textbf{dropout}, which randomly sets some of the activations in the neural net to zero during each training. Activations are the values that connect one layer to another. Dropout forces the neural network to learn redundant internal representations of the data. It sounds inefficient, but it prevents overfitting and makes the model more robust. Essentially, the network becomes a consensus of many networks. If dropout doesn't help, the network probably needs to be larger. 

Of course, the randomness of dropout is undesirable when evaluating a network that has been trained with dropout. You don't want randomness to affect the prediction. The network is written to take the consensus of all the evaluations during evaluation. This is done by scaling the non-zero outputs during training by a factor of $c$ for a dropout rate of $\frac{1}{c}$, for example. 

%------------------------------------------------
%Subsection: Models for Structured Data
\subsection{Models for Structured Data}

Simple deep neural networks are great, but the performance of deep models can be improved if the data are structured like an image or sequence. If the data have a structure, you can simplify the model training by building that structure into the neural network to begin with. 

One concept that is important when considering structures in data is \textbf{statistical invariance}. As an example, consider a model that is designed to look for cats in photographs. It doesn't matter \textit{where} the cat appears in the photo, just that the cat appears somewhere. In this position, we want our neural network to have translational invariance (different position, same kitteh). \textbf{Statistical invariants} are things that don't change on average across time and or space. 

The search for statistical invariants will lead to \textbf{convolutional neural networks} for images and \textbf{embeddings} and \textbf{recurrent neural networks} for text and speech. 

%------------------------------------------------
%Subsection: Convolutional Neural Networks
\subsection{Convolutional Neural Networks} 

\textbf{Convolutional neural networks} (CNNs, or convnets) share their parameters across space. Images have height, width, and depth (color), and nearby patches of images are related. 

Imagine running a patch of an image and running a neural network on it. Then slide the NN across the image, patch by patch, without changing the weights. The output that the neural network will have produced is also an image, just one with a different width, heigh, and depth!

In a convolutional neural network, we have stacks of convolutions. The convolutions will form a pyramid, with the original image at the bottom. The original image is large spatially but relatively shallow. The successive convolutional layers will squeeze the image but also make it deeper. A classifier can be placed at the top of the pyramid. After all of the squeezing and deepening, only features corresponding to the classification task should remain. Depth in the network correponds approximately to \textit{semantic representation}.

Below are some definitions for convnets. 

\begin{itemize}
	\item \textbf{Kernel}: a patch of a convnet. 
	\item \textbf{Feature map}: each "pancake" in the stack of convnets. 
	\item \textbf{Stride}: the number of pixels moved each time the filter is shifted. 
	\item \textbf{Padding}: the technique for dealing with the edges of images.
\end{itemize}

\textbf{Valid padding} is where the convnet never goes past the edge of the image. As a result, the output image is necessarily smaller than the input image. \textbf{Same padding} allows patches to go off the edges of the image. The missing inputs are simply padded with zeros. It is called same padding because the output size is the same as the input size, assuming a stride of 1. 

A few things can be done to improve the CNN architecture. \textbf{Pooling} is a better way to reduce the spatial extent of the features map in the convnet pyramid. As mentioned before, large strides (large pixel shifts between kernels) are a very aggressive way of down-sizing the image. The pooling method is less aggressive: small strides are used, but the convolutions in a neighborhood are combined. \textbf{Max-pooling} uses the maximum output from a neighborhood of outputs. One advantages to max-pooling is that it does not add to the number of model parameters. However, it still has a small stride and is expensive to compute. It also adds to the number of hyper-parameters: pooling size and pooling stride must be tuned. 

A typical network has the following structure: 

\begin{equation}
	\text{image} \rightarrow \text{conv.} \rightarrow \text{max-pool} \rightarrow \text{conv.} \rightarrow \text{max-pool} \rightarrow \text{fully-conn.} \rightarrow \text{classifier}.
\end{equation}

There are other examples of pooling, such as \textbf{average-pooling}, which takes the average value in a neighborhood instead of a max value. It is similar to providing a blurred view. 

Another improvement to the CNN architecture comes from \textbf{$1\times1$ convolutions}. Why would we want to do a $1\times1$ convolution? They just look at 1 pixel and not a patch, after all. The reason is that, if you add a $1\times1$ convolution into the middle of the network, you have a mini-deep network over the patch instead of a linear classifier. So it is an \textit{inexpensive} way to make models deeper with more parameters without changing their structure. $1\times1$ are really just matrix multiplications and not convolutions. 

A third way to improve the CNN architecture is via the \textbf{inception module}. At each layer of the convnet, we have many options when it comes to pooling or convolution. All of these are beneificial and provide modeling power, so why choose between them? We can just use all of the operations and concatenate the output. The combined output of average pooling, $1\times1$, $3\times3$ , and $5\times5$ convolutions, etc. often performs better than a simple convolution.


