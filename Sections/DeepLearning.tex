%----------------------------------------------------------------------------------------
%	Deep Learning
%----------------------------------------------------------------------------------------
\section{Deep Learning}

This section discusses the basics of deep learning. The material is based largely on notes from the Google Udacity course "\href{https://www.udacity.com/course/deep-learning--ud730}{Deep Learning}". On a basic level, deep learning involves training multi-layer neural networks to transform an input data vector into a particular output vector. The networks can be trained in a supervised manner by providing the input data along with the desired output vector, or in an unsupervised way by training the network to build internal, compressed representations of the data in order to reconstruct the input data.

%------------------------------------------------
%Subsection: Softmax
\subsection{Softmax}

In neural networks and other machine learning algorithms, the \textbf{softmax} function is used as the final layer of a classification network. The softmax function is a generalization of the logistic function. It transforms a $K$-dimensional vector $\mathbf{z}$ of arbitrary real values into a $K$-dimensional vector $\sigma(\mathbf{z})$ of real values in the range $(0,1)$ which sum to 1. 

\begin{equation}
\sigma(\mathbf{z})_{j} = \frac{e^{z_{j}}}{\sum_{k=1}^{K} e^{z_{k}}} \text{, for j=1,...,K}
\end{equation}

The result of the softmax function can, for example, be used as the predicted probability for the $j^{th}$ class in a classification task.

Softmax score probabilities get close to 0 or 1 as scores get larger. Similarly, score probabilities get closer to uniform as all scores get smaller. A classifier is less confident in a result with low scores and more confident in a result with high scores. The training of a classifier is designed to increase confidence over time. 

%------------------------------------------------
%Subsection: Initializing variables
\subsection{Initializing variables}

Generally, it is useful to have variables with a mean near zero and equal variance. This will help avoid numerical instability issues and ill-conditioned matrices. Remember: matrix algebra really underpins all of this stuff!

Weight and bias initialization needs to be done carefully, otherwise the training of the model is inefficient. A useful method is to randomly sample from a Gaussian distribution centered at zero with a given standard deviation. The standard deviation is related to the order of magnitude of hte outputs. For a less certain initialization, it is a good idea to use a small standard deviation (much less than 0.1).

%------------------------------------------------
%Subsection: Cross-validation
\subsection{Cross-Validation}

\textbf{Cross-validation}, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent set of data. The idea is that the performance of a model needs to be measured using a dataset that is separate from the one used for training. If the same data are used for training and testing a model, the performance will be falsely inflated, since the model will have trained on some of the statistical variations in the training data. Cross-validation can prevent \textbf{over-fitting}.

A simple cross-validation system involves splitting up the data into a \textbf{training}, \textbf{validation}, and \textbf{testing} dataset. 

\begin{itemize}
	\item Training set: a data sample for training the neural network via back-propagation. 
	\item Validation set: a data sample for validating and measuring the performance.
	\item Testing set: a data sample for preventing over-training. 
\end{itemize}

The bigger the validation set, the more precise the estimate of the model performance. The performance measurement is susceptible to statistical fluctuations, after all. More data always helps. Having a large training dataset is the most important thing. 

%------------------------------------------------
%Subsection: Loss function
\subsection{Loss function}

Machine learning is often about desigining the correct loss-function to use for model optimization. 

In order to train a model, it is necessary to measure how far away the output of a model is from the desired output. One way to do this is to minimize the \textbf{cross-entropy}. Use the average cross-entropy as the loss function.

Sometimes it is difficult to scale the regression model, where the loss function and gradient is computed for each example. The gradient computation, for example, typically takes 3 times as long as the loss computation.

One solution, called \textbf{stochastic gradient descent}, 

%------------------------------------------------
%Subsection: Stochastic Gradient Descent
\subsection{Stochastic Gradient Descent}

Stochastic gradient descent (SGD) involves computing the loss for a tiny random sample of data instead of the whole dataset (between 1 and 1000 training samples each time). This method requires random sampling of the training data, otherwise it won't work. On average, the gradient will point in the right direction. But the approach requires many small steps, each of which might be partially misguided. 

Stochastic gradient descent is at the core of deep learning, since it \textit{scales very well} with data and model size. Bigger data and bigger models are better, after all. However, SGD is a fast but bad optimizer, and it comes with some issues.

First, in order to work properly, the input variables need to have a mean of zero and equal, small, variances. Similarly, the initial weights and biases should have a mean of zero and very small variance. 

The training algorithm can also use knowledge from previous steps to know where it should be going. This method, called \textbf{momentum}, keeps a running average of the gradient direction and uses that for optimization instead of the single gradient measurement. 

It is also useful to make smaller steps as the training continues. This procedure, called \textbf{learning rate decay}, lowers the learning rate over time, typically by exponential damping. 

The learning rate and momentum are examples of \textbf{hyper-parameters} that can be tuned for a model. Some other hyper-parameters include the initial learning rate, the batch size for SGD, and the weight initialization. \textbf{AdaGrad} is a modification of SGD that implicitly does momentum and learning rate decay. It makes learning less sensitive to hyper-parameters, but is slightly worse than tuned SGD with momentum. 

One useful suggestion: \textit{"Keep calm and lower your learning rate"}. 

%------------------------------------------------
%Subsection: Linear Models
\subsection{Linear Models}

A linear classifier creates a discriminant based on the value of a linear combination of the input features. For an input feature vector of real values, $\mathbf{x}$, the output score y is given by:

\begin{equation}
	y = f(\mathbf{w} \cdot \mathbf{x}) = f( \sum_{j} w_{j} x_{j}),
\end{equation}

In this expression, $\mathbf{w}$ is a real vector of weights and $f$ is a function that converts the dot-product of the two vectors into the desired output. $\mathbf{w}$ is a linear function mapping of $\mathbf{x}$ onto $\mathbb{R}$.

Numerically, linear operations are very stable. This means that small changes to the inputs do not lead to big changes in the outputs. The derivatives are constant, so they are also quite stable. The problem is that many problems cannot be represented by a linear model. For example, addition can be modeled, but multiplication cannot be modeled. 

The trick is to create a non-linear model from linear components. We want to keep our parameters inside big linear functions. After all, GPUs are designed for linear operations (e.g. matrix multiplication). And it is really useful to harness GPUs for training. 

%------------------------------------------------
%Subsection: Non-Linear Models
\subsection{Non-Linear Models}

One way to introduce non-linearities into a model is to use \textbf{rectified linear units}. A RELU returns 0 if $x<0$ and $x$ if $x \geq 0$. One advantage of RELU is that the derivative is constant.

A logistic model can then be extended by inserting a RELU layer in the middle: the first layer connects the input features to the RELU, while the second layer connects the RELU to the classifier. The resulting model is nonlinear. 

The first layer of the model consists of the set of weights and biases applied to the input features and passed through the RELUs. The output layer of the RELU layer is fed to the next one, but it is not observable from outside the network. Hence, it is known as a \textbf{hidden} layer. The second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities. 

A network can be built by stacking simple operations on top of each other. It makes the back-propagation math, which is based on the chain rule, very simple! 

%------------------------------------------------
%Subsection: Back-Propagation:
\subsection{Back-Propagation}

\textbf{Back-propagation} is the mechanism for using differences between the desired and actual model output to update the model parameters and improve the prediction. Basically, the loss function is used to compute a gradient for each model parameter using the chain rule. The model parameters (weights and biases) are then updated. As a reminder, the \textbf{chain rule} states that:

\begin{equation}
	[g(f(x))]' = g'(f(x)) \cdot f'(x).
\end{equation}

Note that each block of the back-propagation takes twice the memory and twice the computation time of the forward-propagation. 

%------------------------------------------------
%Subsection: Deep Models
\subsection{Deep Models}

The accuracy of a model can improved by adding more parameters to the model. This could be done by increasing the size of each layer of the model, but this turns out to be inefficient. Increasing the number of nodes per layer makes the network harder to train. A better approach is to make the model deeper by adding more layers. It is more efficient in terms of the number of parameters; you can get more performance with fewer parameters by going deeper rather than wider. Furthermore, deep models capture data with hierarchical structures better. Deeper layers are able to learn more abstract features of the data. 

However, deep models only shine if you have enough data to train them. But it is hard to optimize a network that is the right size. So it is often easier to train a big network and then try to prevent over-fitting. This is sometimes called the "skinny jeans" problem. Skinny jeans look great, but it is hard to fit in them. So it is usually easier to wear jeans that are just a bit too big. 

%------------------------------------------------
%Subsection: Preventing Over-Fitting
\subsection{Preventing Over-Fitting}

One way to prevent over-fitting is \textbf{early termination}. During training, keep track of the validation set performance versus training time. As soon as improvement stops being made on the validation set performance, terminate the training. 

Another way to prevent overfitting is through \textbf{regularization}. With regularization, artificial constraints are applied to the network that implicitly reduce the number of free parameters while not making it more difficult to optimize. 

\textbf{L2 regularization} adds a new term to the loss function that penalizes large weights. The new term is just the sum of the square of the weights multiplied by a new hyperparameter constant $c$: 

\begin{equation}
	Loss' = Loss + \frac{c}{2} (w_{1}^{2} + w_{2}^{2} + ... + w_{M}^{2}).
\end{equation}

L2 regularization is a simple technique that does not change the structure of the network. Furthermore, the derivative of teh L2 norm of the vector is just the weights:

\begin{equation}
	W = w_{1} + w_{2} + ... + w_{M}.
\end{equation}

Perhaps the most important recent technique is called \textbf{dropout}, which randomly sets some of the activations in the neural net to zero during each training. Activations are the values that connect one layer to another. Dropout forces the neural network to learn redundant internal representations of the data. It sounds inefficient, but it prevents overfitting and makes the model more robust. Essentially, the network becomes a consensus of many networks. If dropout doesn't help, the network probably needs to be larger. 

Of course, the randomness of dropout is undesirable when evaluating a network that has been trained with dropout. You don't want randomness to affect the prediction. The network is written to take the consensus of all the evaluations during evaluation. This is done by scaling the non-zero outputs during training by a factor of $c$ for a dropout rate of $\frac{1}{c}$, for example. 

%------------------------------------------------
%Subsection: Models for Structured Data
\subsection{Models for Structured Data}

Simple deep neural networks are great, but the performance of deep models can be improved if the data are structured like an image or sequence. If the data have a structure, you can simplify the model training by building that structure into the neural network to begin with. 

One concept that is important when considering structures in data is \textbf{statistical invariance}. As an example, consider a model that is designed to look for cats in photographs. It doesn't matter \textit{where} the cat appears in the photo, just that the cat appears somewhere. In this position, we want our neural network to have translational invariance (different position, same kitteh). \textbf{Statistical invariants} are things that don't change on average across time and or space. 

The search for statistical invariants will lead to \textbf{convolutional neural networks} for images and \textbf{embeddings} and \textbf{recurrent neural networks} for text and speech. 

%------------------------------------------------
%Subsection: Convolutional Neural Networks
\subsection{Convolutional Neural Networks} 

\textbf{Convolutional neural networks} (CNNs, or convnets) share their parameters across space. Images have height, width, and depth (color), and nearby patches of images are related. 

Imagine running a patch of an image and running a neural network on it. Then slide the NN across the image, patch by patch, without changing the weights. The output that the neural network will have produced is also an image, just one with a different width, heigh, and depth!

In a convolutional neural network, we have stacks of convolutions. The convolutions will form a pyramid, with the original image at the bottom. The original image is large spatially but relatively shallow. The successive convolutional layers will squeeze the image but also make it deeper. A classifier can be placed at the top of the pyramid. After all of the squeezing and deepening, only features corresponding to the classification task should remain. Depth in the network correponds approximately to \textit{semantic representation}.

Below are some definitions for convnets. 

\begin{itemize}
	\item \textbf{Kernel}: a patch of a convnet. 
	\item \textbf{Feature map}: each "pancake" in the stack of convnets. 
	\item \textbf{Stride}: the number of pixels moved each time the filter is shifted. 
	\item \textbf{Padding}: the technique for dealing with the edges of images.
\end{itemize}

\textbf{Valid padding} is where the convnet never goes past the edge of the image. As a result, the output image is necessarily smaller than the input image. \textbf{Same padding} allows patches to go off the edges of the image. The missing inputs are simply padded with zeros. It is called same padding because the output size is the same as the input size, assuming a stride of 1. 

A few things can be done to improve the CNN architecture. \textbf{Pooling} is a better way to reduce the spatial extent of the features map in the convnet pyramid. As mentioned before, large strides (large pixel shifts between kernels) are a very aggressive way of down-sizing the image. The pooling method is less aggressive: small strides are used, but the convolutions in a neighborhood are combined. \textbf{Max-pooling} uses the maximum output from a neighborhood of outputs. One advantages to max-pooling is that it does not add to the number of model parameters. However, it still has a small stride and is expensive to compute. It also adds to the number of hyper-parameters: pooling size and pooling stride must be tuned. 

A typical network has the following structure: 

\begin{equation}
	\text{image} \rightarrow \text{conv.} \rightarrow \text{pool} \rightarrow \text{conv.} \rightarrow \text{pool} \rightarrow \text{fully-conn.} \rightarrow \text{classifier}.
\end{equation}
\\

There are other examples of pooling, such as \textbf{average-pooling}, which takes the average value in a neighborhood instead of a max value. It is similar to providing a blurred view. 

Another improvement to the CNN architecture comes from \textbf{$1\times1$ convolutions}. Why would we want to do a $1\times1$ convolution? They just look at 1 pixel and not a patch, after all. The reason is that, if you add a $1\times1$ convolution into the middle of the network, you have a mini-deep network over the patch instead of a linear classifier. So it is an \textit{inexpensive} way to make models deeper with more parameters without changing their structure. $1\times1$ are really just matrix multiplications and not convolutions. 

A third way to improve the CNN architecture is via the \textbf{inception module}. At each layer of the convnet, we have many options when it comes to pooling or convolution. All of these are beneificial and provide modeling power, so why choose between them? We can just use all of the operations and concatenate the output. The combined output of average pooling, $1\times1$, $3\times3$ , and $5\times5$ convolutions, etc. often performs better than a simple convolution.

%------------------------------------------------
%Subsection: Text in a deep model
\subsection{Text in a deep model} 

How do you classify a document? You can start by having a look at the words in the document. But words are difficult. There are many words, and it is often the most rare words that are the most important. Rare events are a problem for deep learning, which relies on the availability of numerous trainging examples. 

Another problem that text presents: we often use different words to mean the same thing. Cat and kitty, for instance, are often interchangeable. We have to train a model that know that kitty and cat are related. We would like to see important words often enough so that we can learn their meanings automatically and see relationships so that we can share parameters between them. 

The most straightforward approach involves \textbf{unsupervised learning}, or providing training data without labels. It is easy to find text for training on the web or wikipedia, if you can figure out what to learn from it. 

One very powerful idea for training is that \textit{similar words tend to occur in similar contexts}. A model can be built to use words to predict word context. Such a model would have to treat cat and kitty similarly, for instance. And you don't even have to know what individual words mean -- word meanings can be inferred directly from the company they keep. 

To do this, map words to small vectors called \textit{embeddings}. Embedding vectors will be close to each other when words have similar meanings and will be far apart when they have different meanings. Embeddings solve some of the problem with sparsity. Once you have embedded a word into a small vector, you have a word representation where all cat-like things are represented by vectors that are very similar. As a result, the model doesn't have to learn new things for every way of talking about a cat. It can generalize from patterns of cat-like things. 

\textbf{Word2Vec} is a simple and effective way to learn embeddings. Each word of a sentence is mapped to an (initially random) embedding vector. Then the embedding is used to predict the context of the word, where the context is just nearby words. Then the model is trained as if it were a supervised learner. A logistic classifier is used to predict nearby words. 

It would be useful to take a look at the model and see which embeddings are grouped closely together. One option would be to do nearest neighbor lookup for any given word. Or you could reduce the dimensionality of the embedding to 2 dimensions for representation with a principal component analysis (PCA). But much of the useful information is lost in such a compression, so the result is usually not great. \textbf{tSNE} is a visualization technique for embeddings. It provides a way of projecting the embedding that preserves the neighborhood structure. 

For training embeddings, it is also often useful to measure the closeness of two vectors using $\cos()$ and not $L2$. This is because the length of the embedding vector is not nearly as relevant to the classification as direction. In fact, it is often better to normalize all embedding vectors to have unit norm. 

\begin{itemize}
	\item $L2 = ||V_{cat} - V_{kitten}||_{2}^{2}$
	\item $\cos() = \frac{(V_{cat} \cdot V_{kitten})}{||V_{cat}|| ||V_{kitten}||}$
\end{itemize}

Here's how to make an actual model for Word2Vec. Starting with a word, say "cat", embed the word into a small vector $V_{cat}$. Then feed $V_{cat}$ into a simple linear model with weights and biases. That model will output a softmax probability that can be compared to the target, which is another word in the context of the input word. 

The problem is that there are many words in our vocabulary, and computing the softmax over all the words is difficult. One remedy to this is to \textit{randomly sample} the targets. This is called \textbf{sampled softmax}. 

One interesting feature of embeddings is that analogies can be expressed in terms of vector operations. An example of a semantic analogy would be "\textit{puppy - dog + cat = kitty}". An example of a syntactic analogy would be \textit{taller - tall + short = shorter}. 

There are two ways to train an embedding. The \textbf{skip-gram} model uses a word to predict a context. \textbf{CBOW}, or continuous bag of words, uses context to predict words. The two approaches are essentially mirror images of each other. 


%------------------------------------------------
%Subsection: Recurrent Neural Networks
\subsection{Recurrent Neural Networks} 

Of course, there are many way sto learn embeddings. One question: what to do with input sequences of varying lengths? 

\textbf{Recurrent neural networks} use shared parameters across time to extract patterns, and are great for dealing with input sequences. They are similar to convolutional networks, in that they share parameters across some dimension. It's just that CNNs share parameters across \textit{space}, while RNNs share parameters across \textit{time}. 

Imagine a sequence of events. At each point in time, we want the model to make a decision about what has happened so far in the sequence. We want to take into account the past. One option is to use the state of the previous classifier as a summary of the past.

The idea is to use two classifiers: a single model connected to the summary of the past inputs, and a model connected to the inputs from the current step. The network thus has a simple repeating pattern. 

Back-propagation is difficult to do through time, though. The architecture is bad for stochastic gradient descent, since the parameters are correlated. As a result, gradients explode or vanish. Exploding gradients can be handled by a procedure called \textbf{gradient clipping}. The idea is to compute the norm and shrink the step when the norm grows too big. It acts as a cheap and effective cap on gradients. 

Vanishing gradients are a bigger problem. Practically, vanishing gradients are equivalent to memory loss, where the model quickly forgets its summary of past events. The solution to the problem came from a breakthrough called \textbf{LSTM}, or \textbf{long short-term memory}. In the basic recurrent model, a summary of the past $M$ as well as current inputs $X$ are fed into a weight module that produces a prediction $Y$ as well as a summary for the future, $M'$. With LSTM, the weight module is replaced by a LSTM cell. It is still functionally a neural network.

To describe the LSTM, let's first create a simple memory model. \\

\begin{tabular}{p{0.25\textwidth}p{0.25\textwidth}p{0.25\textwidth}}
Instruction & Input & Output \\
\hline
Write & X & M \\
Read & M & Y \\
Forget & M & \\
\end{tabular} \\

Each logic gate can be thought of as a multiplication by 1 or 0. But what if continuous decisions were made at each gate instead, with values between 0 and 1? This would give us a continuous, differentiable, back-prop-able memory model! You can perform a logistic regression of gate values. The gates help the model keep memory longer when needed and forget memory when not needed. 

Regularization techniques are needed for LSTM as with other neural networks. L2 regularization works. So does dropout, as long as you use it on the input or the output but not on the recurrent connections. 

\subsubsection{Prediction with recurrent neural networks}

A model that predicts the next item of a sequence can also be used to \textit{generate} a sequence. This can be done by aternating predictions with samplings to build a sequence. A more sophisticated generative method is to sample multiple times at every step. Then you have multiple sequences (a.k.a. multiple hypotheses) with which you can continue predicting at each step. By looking at the total probability of multiple time steps at a time, you prevent the sampling from accidentally making one bad choice and being stuck with it for the rest of the time sequence. 

One way of doing this is \textbf{beam search}, where only a few of the most likely sequences are kept at each step. This approach works well for generating sequences. It also prevents the number of hypotheses from growing exponentially. 

