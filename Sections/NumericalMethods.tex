%----------------------------------------------------------------------------------------
%	Numerical Methods
%----------------------------------------------------------------------------------------
\section{Numerical Methods}

This section describes a number of methods for solving common problems in scientific programming, such as root-finding, minimization, regression, and matrix algebra. The discussions of each algorithm contain information on the algorithm complexity, the implementation, and the relative merits of the approach. 

Many of the methods discussed provide numerical solutions rather than analytic solutions. This means that results will be approximations, and as such will have an associated error. \textbf{Approximation error} or \textbf{absolute error} is the discrepancy between an exact value and an approximation to that exact value. Such errors can occur because approximations are used instead of real data, or because the measurement of data is imprecise. 

Absolute error is given by:
\begin{equation}
\epsilon = |v-v_{approx.}|
\end{equation}

Relative error is defined:
\begin{equation}
\eta = \frac{\epsilon}{|v|} = |1 - \frac{v_{approx.}}{v}|
\end{equation}

%------------------------------------------------
% Subsection: Root-Finding
\subsection{Root-Finding}

The \textbf{bisection method} is a simple root-finding method that repeatedly bisects an interval and selects the subinterval containing the root for further bisection. Advantages: sthe algorithm is simple to code recursively, and it is robust (always converges). Disadvantages: it is a relatively slow algorithm. 

Given a function $f$ that is continuous on the interval $[a,b]$ with $f(a)$ and $f(b)$ having opposite signs, the bisection method is guaranteed to converge. The absolute error is reduced by half during each application of the method. As a result, the bisection method \textit{converges linearly}. It is significantly slower than other available algorithms. An upper bound on the absolute error (difference between the true and numerical result) can be determined by assuming the worst case scenario, that the root is at the midpoint of the interval $c = (a+b)/2$:

\begin{equation}
|c_{n} - c| \le \frac{|b-a|}{2^{n}}
\end{equation}

where $n$ is the number of iterations. The number of iterations needed to meet a particular error tolerance $\epsilon$ is given by

\begin{equation}
\eta = \log_{2} (\frac{\epsilon_{0}}{\epsilon}) = \frac{\log \epsilon_{0} - \log \epsilon}{\log 2}
\end{equation}

where $\epsilon_{0}$ is the size of the initial interval. The convergence of this algorithm is described as \textit{linear} because $\epsilon_{n+1} = constant \times \epsilon_{n}$. 

Next on the list: Newton-Raphson!

%------------------------------------------------
% Subsection: Minimization
\subsection{Minimization}

%------------------------------------------------
%Subsection: Regression
\subsection{Regression}

Discuss linear regression

Nonlinear regression


%------------------------------------------------
% Subsection: Matrix Algebra
\subsection{Matrix Algebra}

