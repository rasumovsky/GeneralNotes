%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Appropriated by Andrew Hard (15/11/2015)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{General Notes}} % The article title

\author{\spacedlowsmallcaps{Andrew S. Hard\textsuperscript{1}}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{November 15, 2015} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only

\tableofcontents % Print the table of contents

\listoffigures % Print the list of figures

\listoftables % Print the list of tables

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

%\section*{Abstract} % This section will not appear in the table of contents due to the star (\section*)

%\lipsum[1] % Dummy text

%----------------------------------------------------------------------------------------
%	AUTHOR AFFILIATIONS
%----------------------------------------------------------------------------------------

{\let\thefootnote\relax\footnotetext{1 \textit{Department of Physics, University of Wisconsin, Madison, United States of America}}}

%----------------------------------------------------------------------------------------

\newpage % Start the article content on the second page, remove this if you have a longer abstract that goes onto the second page

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction}

As a fifth year graduate student, I have come to the realization that significant portions of my skillset and knowledge base are highly specialized. I intend to pursue a career outside of my field of study (experimental high-energy particle physics). In order to enhance my future job prospects, I decided that it would be useful to review basic concepts in computer science, statistics, mathematics, and machine learning.

%----------------------------------------------------------------------------------------
%	Statistics
%----------------------------------------------------------------------------------------

\section{Basic Statistics}

%------------------------------------------------
% Deduction and Induction
\subsection{Deduction and Induction}

\textbf{Deduction}: "if $A \rightarrow B$ and $A$ is true, then $B$ is true." \\
\textbf{Induction}: "if $A \rightarrow B$ and $B$ is true, then $A$ is more plausible." \\

%------------------------------------------------
% Conditional Probability and Bayes Theorem
\subsection{Conditional Probability and Bayes' Theorem}

The \textbf{conditional probability} of an event $A$ assuming that $B$ has occured, denoted $P(A|B)$, is given by:

\begin{equation}
P(A|B)=\frac{P(A \cap B)}{P(B)}.
\end{equation}

Via multiplication:

\begin{equation}
P(A \cap B) = P(A|B)P(B).
\label{equation:pAUB}
\end{equation}

This makes sense. The probability of $A$ \textit{and} $B$ is the probability of $A$ \textit{given} $B$ times the probability of $B$. Or, just as reasonably, the probability of $A$ \textit{and} $B$ is the probability of $B$ \textit{given} $A$ times the probability of $A$. As an aside, the equation above can be generalized:

\begin{equation}
P(A \cap B \cap C) = P(A|B \cap C) P(B \cap C) = P(A|B \cap C) P(B|C) P(C).
\end{equation}

Back to the derivation, since $P(A \cap B) = P(B \cap A)$, we can use equation \ref{equation:pAUB}:

\begin{equation}
P(A|B)P(B) = P(B|A)P(A).
\end{equation}

And re-arranging gives \textbf{Bayes' Theorem}:
\begin{equation}
P(A|B) = \frac{P(A)P(B|A)}{P(B)} = \frac{P(A)}{P(B)} P(B|A).
\label{equation:simpleBayes}
\end{equation}

% DEFINE prior prob, conditional prob, posterior prob. 
In this equation, $P(A)$ is the \textbf{prior probability}, the initial degree of belief in $A$, or the probability of $A$ before $B$ is observed. It is essentially the probability that hypothesis $A$ is true before evidence is collected. The \textbf{posterior probability} $P(A|B)$ is the degree of belief after taking $B$ into account. 

Bayes' theorem describes the probability of an event, based on conditions that might be relevant to the event. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for evidence (\textbf{Bayesian inference}). Note: in the \textbf{Frequentist Interpretation}, probability measures a proportion of outcomes, not probability of belief. 

Bayes' theorem can also be interpreted in the following manner: $A$ is some hypothesis, and $B$ is the evidence for that hypothesis. 

There is a recursive form of Bayes' theorem, to account for series of data. WHAT IS IT?

%%% EVERYTHING BELOW NEEDS REWORKING

\begin{equation}
S \equiv \bigcup_{i=1}^{N} A_{i},
\end{equation}

so that $A_{i}$ is an event in $S$, and $A_{i} \cap A_{j} = \emptyset$ $\forall$ $i \not= j$. Then:

\begin{equation}
A = A \bigcap S = A \bigcap ( \bigcup_{i=1}^{N} A_{i} ) = \bigcup_{i=1}^{N} (A \cap A_{i})
\end{equation}

Then we can compare the probabilities using the Law of Total Probability:

\begin{equation}
P(A) = P(\bigcup_{i=1}^{N} (A \cap A_{i}))=\sum_{i=1}^{N} P(A \cap A_{i}) = \sum_{i=1}^{N} P(A_{i}) P(A|A_{i})
\end{equation}

Using substitution into Bayes' Theorem (equation \ref{equation:simpleBayes}), we have a recursive form of Bayes' Theorem:

\begin{equation}
P(A_{j} | A) = \frac{P(A_{j}) P(A | A_{j})}{\sum_{i=1}^{N} P(A_{i}) P(A|A_{i})}.
\end{equation}

% CORRELATION VS COVARIANCE

%----------------------------------------------------------------------------------------
%	Data Structures and Associated Algorithms
%----------------------------------------------------------------------------------------
\section{Data Structures and Associated Algorithms}
\subsection{Lists}
\subsection{Trees}
\subsection{Graphs}

%----------------------------------------------------------------------------------------
%	Numerical Methods
%----------------------------------------------------------------------------------------
\section{Numerical Methods}

%----------------------------------------------------------------------------------------
%	Machine Learning Introduction
%----------------------------------------------------------------------------------------
\section{Machine Learning Introduction}

This section should contain a general description of data science principles, as well as use cases for different algorithms. Many tools for many problems. 

Basic problems in machine learning are classification (labeling) and regression (function estimation). 

How to do importance ranking of features? Feature importance, global loss function.
%----------------------------------------------------------------------------------------
%	Feature Selection
%----------------------------------------------------------------------------------------
\section{Feature Selection}

Garbage in gives garbage out. 
Interactions between features. Interactions are not the same as correlations.
Filters versus wrappers.

%----------------------------------------------------------------------------------------
%	Clustering
%----------------------------------------------------------------------------------------
\section{Clustering}

%----------------------------------------------------------------------------------------
%	Regression
%----------------------------------------------------------------------------------------
\section{Regression}

%----------------------------------------------------------------------------------------
%	Support Vector Machines
%----------------------------------------------------------------------------------------
\section{Support Vector Machines}

%----------------------------------------------------------------------------------------
%	Decision Trees
%----------------------------------------------------------------------------------------
\section{Decision Trees}

%----------------------------------------------------------------------------------------
%	Neural Networks
%----------------------------------------------------------------------------------------
\section{Neural Networks}
\subsection{Use cases, overview}
\subsection{MLP: Multi-Layer Perceptrons}
\subsection{RBM: Restricted Boltzmann Machines}
\subsection{CNN: Convolutional Neural Networks}
\subsection{RNN: Recurrent Neural Networks}
\subsection{LSTM: Long Short Term Memory}
\subsection{Neural Turing Machines}

%----------------------------------------------------------------------------------------
%	Auto-Encoders and Deep Belief Networks
%----------------------------------------------------------------------------------------
\section{Auto-Encoders and Deep Belief Networks}

%----------------------------------------------------------------------------------------
%	Lasso Method
%----------------------------------------------------------------------------------------
\section{Lasso Method}

%\begin{description}
%\item[Word] Definition
%\item[Concept] Explanation
%\item[Idea] Text
%\end{description}

%\begin{itemize}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
%\item First item in a list
%\item Second item in a list
%\item Third item in a list
%\end{itemize}

%Reference to Table~\vref{tab:label}. % The \vref command specifies the location of the reference

%------------------------------------------------

%\subsection{Figure Composed of Subfigures}

%Reference the figure composed of multiple subfigures as Figure~\vref{fig:esempio}. Reference one of the subfigures as Figure~\vref{fig:ipsum}. % The \vref command specifies the location of the reference

%\begin{figure}[tb]
%\centering
%\subfloat[A city market.]{\includegraphics[width=.45\columnwidth]{Lorem}} \quad
%\subfloat[Forest landscape.]{\includegraphics[width=.45\columnwidth]{Ipsum}\label{fig:ipsum}} \\
%\subfloat[Mountain landscape.]{\includegraphics[width=.45\columnwidth]{Dolor}} \quad
%\subfloat[A tile decoration.]{\includegraphics[width=.45\columnwidth]{Sit}}
%\caption[A number of pictures.]{A number of pictures with no common theme.} % The text in the square bracket is the caption for the list of figures while the text in the curly brackets is the figure caption
%\label{fig:esempio}
%\end{figure}

%----------------------------------------------------------------------------------------
%	References
%----------------------------------------------------------------------------------------

\renewcommand{\refname}{\spacedlowsmallcaps{References}} % For modifying the bibliography heading

\bibliographystyle{unsrt}

\bibliography{bibliography.bib} % The file containing the bibliography

%----------------------------------------------------------------------------------------

\end{document}